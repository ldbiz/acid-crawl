\documentclass{acm_proc}
\usepackage{xcolor}
\usepackage{listings}
%\usepackage{hyperref}
\usepackage[hyphens]{url}
\usepackage[hidelinks]{hyperref}
%\usepackage{url}
\usepackage{float}

\usepackage{lipsum}
\usepackage{courier}
\usepackage{datetime}
\usepackage{graphicx, subfigure}
\usepackage{stfloats}

\usepackage[T2A,T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian,english]{babel}
\usepackage{array}

\usepackage{newunicodechar}
\newunicodechar{≈ù}{\^{s}} % handy shortcut

%\usepackage{CJKutf8} %for Chinese characters
\usepackage{lmodern}
\usepackage{algpseudocode} 

\hyphenation{Java-Script}
\hyphenation{name-ly}

\usepackage[shortlabels]{enumitem}


\lstset{ %
	basicstyle=\tiny\ttfamily,
	breaklines=true	
}

\newcommand{\todo}[1]{{\textit{\color{red}#1}}}
\newcommand*{\tabbox}[2][t]{%
    \vspace{0pt}\parbox[#1][3.7\baselineskip]{1cm}{\strut#2\strut}}
\newcommand{\specialcell}[2][c]{%
  \begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}

% Make references to specific tests available
\usepackage{hyperref}
\usepackage{nameref}

\makeatletter
\let\orgdescriptionlabel\descriptionlabel
\renewcommand*{\descriptionlabel}[1]{%
  \let\orglabel\label
  \let\label\@gobble
  \phantomsection
  \edef\@currentlabel{#1}%
  %\edef\@currentlabelname{#1}%
  \let\label\orglabel
  \orgdescriptionlabel{#1}%
}
\makeatother

    
\begin{document}

\title{The Archival Acid Test:\\A First Step in the Evaluation of Archival Crawlers\\
\large{\vspace{0.5em}\todo{DRAFT as of \today\ \currenttime}}
\vspace{-2.0em} % Extra space to remove impact of draft message
}


\numberofauthors{1} 
\author{
\alignauthor
Mat Kelly, Michael L. Nelson, and Michele C. Weigle\\
	   \affaddr{Old Dominion University}\\
       \affaddr{Department of Computer Science}\\
       \affaddr{Norfolk, Virginia 23529 USA}\\
       \email{\{mkelly,mln,mweigle\}@cs.odu.edu }
}


\maketitle
\begin{abstract}

abstract

\end{abstract}


\terms{Lorem, Ipsum}

\keywords{Foo, Bar, Baz} % NOT required for Proceedings
\\
\\
\section{Introduction}
When archiving the web, archival crawlers produce a result that, when viewed by the end-user, frequently varies from what they expected. To evaluate the quantitative degree to which an archival crawler is capable of comprehensively reproducing a web page from the live web into the archives, the crawlers capabilities must be evaluated. In this paper, we propose some various metrics to evaluate the capability of an archival crawler and other preservation tools using the Acid Test concept. For a variety of web preservation tools, we examine previous captures within web archives and note the features that produce incomplete or unexpected results. From there, we design the test to produce a quantitative measure of how well each tool performs its task.

\begin{itemize}
\item More on motive here
\item Why do/n't acid tests work (\& how have they with browsers of the past)?
\end{itemize}

\section{Related Work}

%Current methods of web archiving can be broken down into software and services.

Web archives are generated by a variety of tools in a variety of format. An ISO standard format utilized by institutional and personal web archivists alike is the Web ARChive (WARC) format \cite{isowarc}. WARC files allow HTTP communication that occurred during a crawl as well as payload, metadata and other archival features to be encoded in a single or an extensibly defined set of WARC files.

Mohr's Heritrix paved the way from Internet Archive (IA) to utilize their open source Heritrix to create ARC and WARC files from web crawls while capturing all resources necessary to replay a web page \cite{Heritrix}. Other tools have since added WARC creation functionality \cite{wget-warc,kelly-jcdl2012,warc-proxy}. Multiple software platforms exist that can replay WARCs but IA's Wayback Machine\footnote{\url{http://archive.org/web/}} (and its open source counterpart\footnote{\url{https://github.com/iipc/openwayback}}) is the de facto standard.

Multiple services exist that allow users to submit URIs for preservation. IA recently began offering a ``Save Page Now'' feature co-located with their web archive browsing interface. WebCitation\footnote{\url{http://www.webcitation.org/}}, Mummify\footnote{\url{https://mummify.it/}}, and archive.is\footnote{\url{http://archive.is}} all provide URI submission and archive browsing services (with obfuscated URIs\footnote{\todo{these aren't opaque but is there a better term?}}) though none make accessible a resultant WARC file. \todo{(others to add?)}

\begin{itemize}

\item WARC
\item Playback Systems\begin{itemize}
	\item Wayback
	\item warc-tools
	\item warc-explorer (might be based on warc-tools)
\end{itemize}
\item Acid Tests (particularly the web-based variety)
\item Source in capturing mobile web (Frank's? Mat+JFB's dlib?)
\item Source in capturing content behind authentication / currently un-capturable by crawlers
\item YouTube/multimedia capture (also, how in the world can these be evaluated automatically?)
\item Resources within resource within resources (thinking: JS includes, stylesheets)
\item Leakage (Zombies) and false-positive examples already in the archives
\end{itemize}

\section{The Archival Acid Test}
\todo{Primer on what the (web standards) acid test is/was, how it's used, how it's applicable to crawlers\\}
Inspired by the Acid Tests administered by the Web Standard Project (WaSP)\footnote{\url{http://www.acidtests.org/}}, we built the Archival Acid Test to evaluate how well archival tools perform at preserving web pages. Unlike WaSP's initiatives, evaluation of web archival software is not standardized, so a comprehensive test of what these tools should be able to capture should be established. Given that tools to preserve the web must first obtain a representation \cite{rfc2616} of the target resources prior to saving the representation, the archival acid test evaluates the result of the tools' preservation mechanism with the basis of the result being replayed, i.e., the archive being viewed.

\subsection{The Tests}
\todo{For each test, give justification for it (why it's important), an instance where the test fails with current software and how it's implemented\\}
The crux of the tests is to determine how well an archival tool preserves a web page in terms of similarity of what would be expected by a user viewing the page from the live web, i.e., a respectively modern web browser.  Web standards are continuously evolving with the feature set for web browsers temporally lags in the standards in being implemented though frequently containing experimental implementations. Archival crawlers, given a greater need for reliability, lag further in implementing newly standardized features than browsers though will frequently rely on a common engine utilized by browsers to stay-up-date.\footnote{For example, the open source V8 and SpiderMonkey rendering engines are utilized to allow resources that require JavaScript to be present on a web page and able to be captured by archival tools.} The deviation from archival tools using browser engines is a source of discrepancy between the content on a live web page and that which is captured by these tools.

We have broken a set of tests into three categories to better group web page features that would be problematic for archival tools to capture. 

\subsection{Basic Tests (Group 1)}

The set of {\em Basic Tests} is meant to ensure that simple representations of resources on web pages are captured. Each tests' name represents what is presented to be captured by the archival crawler. An sample URI follows each test's name.

\vspace{-1.0em}
\begin{description}\itemsep0pt \parskip0pt \parsep0pt
\item[1a.] Local (same server as test) image, relative URI to test \\ \url{./1a.png}
\item[1b.] Local image, absolute URI \\ \url{http://archiveacidtest/1b.png}
\item[1c.\label{itm:test1c}] Remote image, absolute URI \\ \url{http://anotherserver/1c.png} %\item[1c.\label{itm:test1c}] 
\item[1d.] Inline content, encoded image \\ \url{data:image/png;base64,iVB...}
\item[1e.] Remote image, scheme-less URI \\ \url{//http://anotherserver/1e.png}
\item[1f.] Recursively included CSS\\ In style.css: \texttt{@import url(''1f.css'');}
\end{description}

\subsection{JavaScript Tests (Group 2)}

The second group of tests is meant to evaluate the archival crawler's JavaScript support in terms of how the script would execute were the test accessed on the live web with a browser.

\vspace{-1.0em}
\begin{description}\itemsep0pt \parskip0pt \parsep0pt
\item[2a.] Local script, relative URI, loads local resource\\\texttt{<script src="}\url{local.js}\texttt{" />}
\item[2b.] Remote script, absolute URI, loads local resource\\\texttt{<script src="}\url{http://anotherserver/local.js}\texttt{" />}
\item[2c.] Inline script, manipulates DOM\footnote{Document Object Model, the structure of the web page that, when manipulated, affects the content} at runtime\\\texttt{<script>...(JS code)...</script>}
\item[2d.] Inline script, Ajax image replacement, loads local resource\\\texttt{img.src = ''incorrect.png'';\\...code to replace incorrect image with local...}
\item[2e.] Inline script, Ajax image replacement, Same-origin Policy (SOP)\footnote{\url{https://developer.mozilla.org/en-US/docs/Web/JavaScript/Same_origin_policy_for_JavaScript}} enforcement, replacement (bad) == false positive\\\texttt{img.src = ''correct.png'';\\...code to replace correct image with img from SOP violation...}
\item[2f.] Inline script, only served to browser-based user agents (e.g., !wget)
\item[2g.] Inline script, manipulates DOM after delay\\\texttt{setTimeout(function()\{ ...load image...\},2000);}
\item[2h.] Runtime script inclusion\\\texttt{setTimeout(function()\{ ...document.write script...\},2100);}
\item[2i.] Inline script, content loaded upon interaction, introducing resources\\\texttt{window.onscroll = function(){}}
\item[2j.] Inline script, add local CSS at runtime
\end{description}

\subsection{Advanced Features (Group 3)}

The third group of tests evaluates script-related features of HTML beyond simple DOM manipulation.

\vspace{-1.0em}
\begin{description}\itemsep0pt \parskip0pt \parsep0pt
\item[3a.] HTML5 Canvas drawing with runtime-fetched content
\item[3b.] Remote image stored then retrieved from localStorage
\item[3c.] Embedded local content using iframe
\item[3d.] Embedded remote content using iframe
\item[3e.] Cross-Origin Resource Sharing (CORS), enabled
\item[3f.] XSS (CORS denied), for false positive
\item[3g.] Runtime binary object
\end{description}

\section{Target Web Preservation Tools}
\todo{Talk about specific implementations of the tools, environments required, scope, audience intention, general capabilities, etc.\\}

\section{Evaluation}
\todo{What are we going to evaluate?\\}

\todo{How do these archival tools do at WaSP's acid tests?}

\subsection{Tools' Performance}
\todo{How'd they do?\\}
Mummi.fy
- fails 2d (Ajax replacements), 2i (content loaded after interaction), 2j (code that dynamically adds stylesheets)
- leaks to live web with WhichBrowser script

archive.is
- fails at 2i (Code that loads content only after user interaction)
- strange, all images converted to data URIs

perma.cc
- fails 2d (Ajax replacements), 2i (content loaded after interaction), 2j (code that dynamically adds stylesheets), 3b (localStorage)
- leaks to browser detect script
- 
\\
\todo{\\Here be plots.\\}

\subsection{Evaluating the Acid Test's Methods}
\todo{Is this testing what we think it's testing? Where are instances that show or disprove the analysis done by the tests?\\}

\section{Conclusions}
\begin{itemize}
\item What did we accomplish here?
\item What can the preservation tools do to pass the tests and thus ``archive better''?
\end{itemize}

\section{Future Work}
\todo{Things not possible to measure though the crawlers definitely aren't capturing}

\bibliographystyle{plain}
\bibliography{dl2014acid}  


\end{document}
